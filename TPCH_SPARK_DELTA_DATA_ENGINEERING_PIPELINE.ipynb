{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "5hxir3h4kqcncbwtynr3",
   "authorId": "323727975498",
   "authorName": "JOHN",
   "authorEmail": "nick.akincilar@snowflake.com",
   "sessionId": "11a6ee6c-ae6b-4bbf-93c1-c09404f85559",
   "lastEditTime": 1758900260401
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cbabf-9e28-4a53-9be8-170eb3ba1589",
   "metadata": {
    "name": "cell28",
    "language": "python"
   },
   "outputs": [],
   "source": "## CONFIG\n# Select runtime mode and I/O namespaces\n# RUN_MODE options: \"spark\" or \"snowpark_connect\"\nRUN_MODE = \"snowpark_connect\"\n\n# Input (source) and Output (target) namespaces\nSOURCE_DATABASE = \"SNOWFLAKE_SAMPLE_DATA\"\nSOURCE_SCHEMA   = \"TPCH_SF100\"\nTARGET_DATABASE = \"DELETETHIS\"\nTARGET_SCHEMA   = \"TEST\"\n\n# CDC parameters\nCDC_SAMPLE_SIZE = 10_000\nRANDOM_SEED = 42\n\nprint(f\"RUN_MODE = {RUN_MODE}\")\nprint(f\"Source  = {SOURCE_DATABASE}.{SOURCE_SCHEMA}\")\nprint(f\"Target  = {TARGET_DATABASE}.{TARGET_SCHEMA}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbd9e9-cf84-4f5e-8a59-07aa4bc0b9b9",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e9cd9-de1c-42ba-b168-df91cff357ad",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "\n",
    "# TPCH → Delta (Open-Source Spark) — Sample Data Engineering Notebook\n",
    "\n",
    "Now with **configurable database & schema** for both **source** and **target** table locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c4cc-350b-40a3-b9b1-e4806c65dffe",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## 0) Environment & Spark Session (Delta Lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e78e24b-e37c-46a2-8b09-9dabbc548f09",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "if RUN_MODE == \"spark\":\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"tpch-delta-pipeline-sample\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d4b83-6b47-429c-bfc6-2b74abc4f844",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "if RUN_MODE == \"snowpark_connect\":\n    !pip install jdk4py\n    !pip install deltalake\n    !pip install snowpark-connect==0.28.0\n    \n    # Restart the KERNEL via the UI in SnowflakeNOtebook SPCS after initial install.\n\n    from snowflake import snowpark_connect\n    from deltalake import DeltaTable\n    spark = snowpark_connect.server.init_spark_session()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba6796-460a-45ac-a4b3-61c0d7fdc416",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Clears all RDDs, DataFrames, and temporary views persisted in memory or disk.\n",
    "spark.catalog.clearCache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc42f0a-73cc-4d5e-b529-6a2ad675e74b",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## 1) Initialize pipeline timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33448d4c-dd6e-4d1b-af99-2f6015810727",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "\nfrom datetime import datetime\n\npipeline_start_ts = datetime.now()\nprint(\"Pipeline started at:\", pipeline_start_ts.isoformat(timespec=\"seconds\"))\n"
  },
  {
   "cell_type": "markdown",
   "id": "c0a6862a-75f2-4ebd-becd-aaf430d68898",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## 2) Helpers: quoting & fully-qualified names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd3877-17ee-4369-a677-3e7cdbb8ac60",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def q(ident: str) -> str:\n",
    "    \"\"\"Backtick-quote an identifier, handling dots/backticks safely.\"\"\"\n",
    "    if ident is None or ident == \"\":\n",
    "        return \"\"\n",
    "    ident = ident.replace(\"`\", \"``\")\n",
    "    return f\"`{ident}`\"\n",
    "\n",
    "def fqtn(database: str, schema: str, table: str) -> str:\n",
    "    \"\"\"Build a fully-qualified table name using database & schema.\n",
    "\n",
    "    - If database and schema are provided: `database`.`schema`.`table`\n",
    "\n",
    "    - If only database is provided: `database`.`table`\n",
    "\n",
    "    - If neither provided: just `table`\n",
    "\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    if database:\n",
    "        parts.append(q(database))\n",
    "    if schema:\n",
    "        parts.append(q(schema))\n",
    "    parts.append(q(table))\n",
    "    return \".\".join(parts)\n",
    "\n",
    "def ensure_namespace(database: str, schema: str):\n",
    "    \"\"\"Create database/schema (namespace) if not exists.\"\"\"\n",
    "    if database and schema:\n",
    "        # Two-level namespace\n",
    "        spark.sql(f\"CREATE NAMESPACE IF NOT EXISTS {q(database)}.{q(schema)}\")\n",
    "    elif database:\n",
    "        # Single-level (Hive-style)\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {q(database)}\")\n",
    "\n",
    "def df_lower_columns(df: DataFrame) -> DataFrame:\n",
    "    return df.toDF(*[c.lower() for c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e894b2-6af4-4609-ad89-9e529f0631ca",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {fqtn(TARGET_DATABASE, TARGET_SCHEMA, 'dim_customer')}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {fqtn(TARGET_DATABASE, TARGET_SCHEMA, 'fact_orders')}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {fqtn(TARGET_DATABASE, TARGET_SCHEMA, 'agg_customer_revenue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6be96-214d-4055-96ca-1ea6227eeafd",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## 3) Read TPCH Source Delta Tables & Lower-case Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178243ad-bb7b-4322-804d-163e6b4025ba",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "required_tables = [\n",
    "    \"customer\", \"orders\", \"lineitem\",\n",
    "    \"part\", \"supplier\", \"partsupp\",\n",
    "    \"nation\", \"region\"\n",
    "]\n",
    "\n",
    "# Ensure target namespace exists\n",
    "ensure_namespace(TARGET_DATABASE, TARGET_SCHEMA)\n",
    "\n",
    "dfs = {}\n",
    "for t in required_tables:\n",
    "    full = fqtn(SOURCE_DATABASE, SOURCE_SCHEMA, t)\n",
    "    print(\"Reading:\", full)\n",
    "    df = spark.table(full)\n",
    "    df = df_lower_columns(df)\n",
    "    dfs[t] = df\n",
    "\n",
    "# Optional peek (comment out count for large datasets)\n",
    "for name, df in list(dfs.items())[:3]:\n",
    "    print(name, \"→\", df.columns[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00e0b3-25fc-4498-abcd-3ff467c2847b",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## 4) Transformations: Complex Joins + Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858b728-1902-4b90-81c8-31ee9aec6466",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "customer = dfs[\"customer\"]\n",
    "orders = dfs[\"orders\"]\n",
    "lineitem = dfs[\"lineitem\"]\n",
    "nation = dfs[\"nation\"]\n",
    "region = dfs[\"region\"]\n",
    "\n",
    "cust_geo = (\n",
    "    customer.alias(\"c\")\n",
    "    .join(nation.alias(\"n\"), F.col(\"c.c_nationkey\") == F.col(\"n.n_nationkey\"), \"left\")\n",
    "    .join(region.alias(\"r\"), F.col(\"n.n_regionkey\") == F.col(\"r.r_regionkey\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"c.c_custkey\").alias(\"custkey\"),\n",
    "        F.col(\"c.c_name\").alias(\"name\"),\n",
    "        F.col(\"c.c_acctbal\").alias(\"acctbal\"),\n",
    "        F.col(\"c.c_mktsegment\").alias(\"mktsegment\"),\n",
    "        F.col(\"n.n_name\").alias(\"nation\"),\n",
    "        F.col(\"r.r_name\").alias(\"region\")\n",
    "    )\n",
    ")\n",
    "\n",
    "order_revenue = (\n",
    "    lineitem.groupBy(\"l_orderkey\")\n",
    "    .agg(F.sum(F.col(\"l_extendedprice\") * (1 - F.col(\"l_discount\"))).alias(\"order_revenue\"))\n",
    ")\n",
    "\n",
    "orders_enriched = (\n",
    "    orders.alias(\"o\")\n",
    "    .join(order_revenue.alias(\"rev\"), F.col(\"o.o_orderkey\") == F.col(\"rev.l_orderkey\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"o.o_orderkey\").alias(\"orderkey\"),\n",
    "        F.col(\"o.o_custkey\").alias(\"custkey\"),\n",
    "        F.col(\"o.o_orderstatus\").alias(\"orderstatus\"),\n",
    "        F.col(\"o.o_totalprice\").alias(\"totalprice\"),\n",
    "        F.col(\"o.o_orderdate\").alias(\"orderdate\"),\n",
    "        F.col(\"o.o_orderpriority\").alias(\"orderpriority\"),\n",
    "        F.col(\"o.o_clerk\").alias(\"clerk\"),\n",
    "        F.col(\"o.o_shippriority\").alias(\"shippriority\"),\n",
    "        F.col(\"rev.order_revenue\").alias(\"order_revenue\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_latest = Window.partitionBy(\"custkey\").orderBy(F.col(\"orderdate\").desc())\n",
    "w_revenue_rank = Window.partitionBy(\"custkey\").orderBy(F.col(\"order_revenue\").desc_nulls_last())\n",
    "\n",
    "orders_w = (\n",
    "    orders_enriched\n",
    "    .withColumn(\"rn_latest\", F.row_number().over(w_latest))\n",
    "    .withColumn(\"revenue_rank\", F.dense_rank().over(w_revenue_rank))\n",
    ")\n",
    "\n",
    "w_rolling = (\n",
    "    Window.partitionBy(\"custkey\")\n",
    "    .orderBy(F.col(\"orderdate\").cast(\"timestamp\").cast(\"long\"))\n",
    "    .rangeBetween(-30 * 24 * 3600, 0)\n",
    ")\n",
    "\n",
    "orders_w = orders_w.withColumn(\n",
    "    \"rolling_30d_revenue\",\n",
    "    F.sum(\"order_revenue\").over(w_rolling)\n",
    ")\n",
    "\n",
    "#orders_w.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fa08f-f07d-4463-97af-3d37b7856af6",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## 5) Create Curated Target Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78dfa2-6d07-43b9-a092-7f170718839e",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "DIM_CUSTOMER_TBL = fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"dim_customer\")\n",
    "FACT_ORDERS_TBL  = fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"fact_orders\")\n",
    "AGG_REVENUE_TBL  = fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"agg_customer_revenue\")\n",
    "\n",
    "dim_customer_df = (\n",
    "    cust_geo.dropDuplicates([\"custkey\"])\n",
    ")\n",
    "\n",
    "fact_orders_df = orders_w.select(\n",
    "    \"orderkey\", \"custkey\", \"orderstatus\", \"totalprice\", \"orderdate\",\n",
    "    \"orderpriority\", \"clerk\", \"shippriority\", \"order_revenue\",\n",
    "    \"rn_latest\", \"revenue_rank\", \"rolling_30d_revenue\"\n",
    ")\n",
    "\n",
    "agg_revenue_df = (\n",
    "    fact_orders_df.groupBy(\"custkey\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"orderkey\").alias(\"order_cnt\"),\n",
    "        F.sum(\"order_revenue\").alias(\"lifetime_revenue\"),\n",
    "        F.max(\"orderdate\").alias(\"last_order_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "for t in [DIM_CUSTOMER_TBL, FACT_ORDERS_TBL, AGG_REVENUE_TBL]:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {t}\")\n",
    "\n",
    "dim_customer_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(DIM_CUSTOMER_TBL)\n",
    "fact_orders_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(FACT_ORDERS_TBL)\n",
    "agg_revenue_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(AGG_REVENUE_TBL)\n",
    "\n",
    "print(\"Created/overwritten:\")\n",
    "for t in [DIM_CUSTOMER_TBL, FACT_ORDERS_TBL, AGG_REVENUE_TBL]:\n",
    "    print(\" -\", t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf34832-daa1-476c-be82-a0730921c232",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "## 6) Simulate Incremental CDC (10K rows) and Apply to FACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02db5da-2cf0-4427-8955-1c45152edb46",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "sim_src = (\n",
    "    orders_enriched\n",
    "    .withColumn(\"rnd\", F.rand(seed=RANDOM_SEED))\n",
    "    .withColumn(\"rn\", F.row_number().over(Window.orderBy(\"rnd\")))\n",
    "    .filter(F.col(\"rn\") <= CDC_SAMPLE_SIZE)\n",
    "    .drop(\"rnd\",\"rn\")\n",
    ")\n",
    "\n",
    "sim_cdc = (\n",
    "    sim_src\n",
    "    .withColumn(\"p\", F.rand(seed=RANDOM_SEED))\n",
    "    .withColumn(\n",
    "        \"op\",\n",
    "        F.when(F.col(\"p\") < 0.30, F.lit(\"I\"))\n",
    "         .when(F.col(\"p\") < 0.60, F.lit(\"D\"))\n",
    "         .otherwise(F.lit(\"U\"))\n",
    "    )\n",
    "    .drop(\"p\")\n",
    ")\n",
    "\n",
    "INSERT_KEY_OFFSET = 10_000_000\n",
    "sim_cdc = sim_cdc.withColumn(\n",
    "    \"merge_orderkey\",\n",
    "    F.when(F.col(\"op\") == \"I\", F.col(\"orderkey\") + F.lit(INSERT_KEY_OFFSET)).otherwise(F.col(\"orderkey\"))\n",
    ")\n",
    "\n",
    "sim_cdc = sim_cdc.withColumn(\n",
    "    \"orderpriority\",\n",
    "    F.when(F.col(\"op\") == \"U\", F.concat_ws(\"-\", F.col(\"orderpriority\"), F.lit(\"upd\"))).otherwise(F.col(\"orderpriority\"))\n",
    ")\n",
    "\n",
    "sim_cdc = sim_cdc.select(\n",
    "    F.col(\"merge_orderkey\").alias(\"orderkey\"),\n",
    "    \"custkey\", \"orderstatus\", \"totalprice\", \"orderdate\",\n",
    "    \"orderpriority\", \"clerk\", \"shippriority\", \"order_revenue\", \"op\"\n",
    ").cache()\n",
    "\n",
    "print(\"CDC counts:\")\n",
    "sim_cdc.groupBy(\"op\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fbe01-762b-443b-a7ec-4fc2aa2a51e5",
   "metadata": {
    "name": "cell16"
   },
   "source": [
    "### 6a) Apply CDC via Delta MERGE (fallback to DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee28f3-44d4-4953-8b31-bb40793551c9",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "# Assuming TARGET_DATABASE, TARGET_SCHEMA, and sim_cdc are defined\n",
    "# NOTE: spark and fqtn must be defined/available in your execution environment.\n",
    "# TARGET_DATABASE, TARGET_SCHEMA, and sim_cdc (a DataFrame) must also be available.\n",
    "FACT_ORDERS_TBL = fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"fact_orders\")\n",
    "\n",
    "# Create the Delta table if it doesn't exist\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {FACT_ORDERS_TBL} (\n",
    "        orderkey BIGINT,\n",
    "        custkey BIGINT,\n",
    "        orderstatus STRING,\n",
    "        totalprice DOUBLE,\n",
    "        orderdate DATE,\n",
    "        orderpriority STRING,\n",
    "        clerk STRING,\n",
    "        shippriority INT,\n",
    "        order_revenue DOUBLE,\n",
    "        rn_latest INT,\n",
    "        revenue_rank INT,\n",
    "        rolling_30d_revenue DOUBLE\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Ensure proper column types\n",
    "spark.sql(f\"ALTER TABLE {FACT_ORDERS_TBL} ALTER COLUMN orderstatus TYPE STRING\")\n",
    "spark.sql(f\"ALTER TABLE {FACT_ORDERS_TBL} ALTER COLUMN orderpriority TYPE STRING\")\n",
    "spark.sql(f\"ALTER TABLE {FACT_ORDERS_TBL} ALTER COLUMN clerk TYPE STRING\")\n",
    "\n",
    "def apply_cdc_operations_merge(source_df, target_table_name):\n",
    "    \"\"\"\n",
    "    Applies Change Data Capture (CDC) operations (I, U, D) to a target table\n",
    "    using a single, atomic MERGE INTO operation.\n",
    "\n",
    "    Args:\n",
    "        source_df (DataFrame): The source DataFrame containing CDC records with 'op' column.\n",
    "        target_table_name (str): The full name of the target Delta table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Prepare the source DataFrame with proper schema alignment\n",
    "    #    This creates the source dataset for MERGE, ensuring it has all 12 target columns.\n",
    "    cdc_data = (\n",
    "        source_df\n",
    "            .withColumn(\"rn_latest\", F.lit(0).cast(\"int\"))\n",
    "            .withColumn(\"revenue_rank\", F.lit(0).cast(\"int\"))\n",
    "            .withColumn(\"rolling_30d_revenue\", F.lit(0.0).cast(\"double\"))\n",
    "            .withColumn(\"orderstatus\", F.substring(F.col(\"orderstatus\"), 1, 10))\n",
    "            .withColumn(\"orderpriority\", F.substring(F.col(\"orderpriority\"), 1, 15))\n",
    "            .withColumn(\"clerk\", F.substring(F.col(\"clerk\"), 1, 15))\n",
    "    )\n",
    "\n",
    "    # 2. 💥 FIX: Dynamically build the column lists, EXCLUDING the 'op' column.\n",
    "    #    This ensures the source and target column counts are equal for INSERT/UPDATE actions.\n",
    "    target_data_columns = [col for col in cdc_data.columns if col != 'op']\n",
    "\n",
    "    # Generate the column assignments for UPDATE and INSERT\n",
    "    update_set_clause = \", \".join([f\"target.{col} = source.{col}\" for col in target_data_columns])\n",
    "    \n",
    "    insert_cols_clause = f\"({', '.join(target_data_columns)})\"\n",
    "    insert_vals_clause = f\"({', '.join([f'source.{col}' for col in target_data_columns])})\"\n",
    "\n",
    "    print(f\"Applying CDC operations using MERGE INTO...\")\n",
    "    \n",
    "    # Create or replace a temporary view for the CDC data\n",
    "    cdc_data.createOrReplaceTempView(\"cdc_source\")\n",
    "\n",
    "    # 3. Use the explicit column lists in the MERGE statement\n",
    "    merge_sql = f\"\"\"\n",
    "    MERGE INTO {target_table_name} AS target\n",
    "    USING cdc_source AS source\n",
    "    ON target.orderkey = source.orderkey\n",
    "    WHEN MATCHED AND source.op = 'D' THEN\n",
    "        DELETE\n",
    "    WHEN MATCHED AND source.op = 'U' THEN\n",
    "        UPDATE SET {update_set_clause}\n",
    "    WHEN NOT MATCHED AND source.op = 'I' THEN\n",
    "        INSERT {insert_cols_clause} VALUES {insert_vals_clause}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        spark.sql(merge_sql)\n",
    "        print(\"CDC operations completed successfully using MERGE!\")\n",
    "    except Exception as e:\n",
    "        print(f\"MERGE failed: {e}\")\n",
    "        # In case of failure, you might want to log the error or handle it gracefully\n",
    "        # depending on your production environment.\n",
    "\n",
    "# Apply the CDC operations\n",
    "apply_cdc_operations_merge(sim_cdc, FACT_ORDERS_TBL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca081bb-91eb-4ca4-abc7-c2a940c55342",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "### 6b) Post-CDC Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59efb0c-fff8-4214-a24b-72c290dfa8ac",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "\n",
    "row_count = spark.table(fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"fact_orders\")).count()\n",
    "print(\"Rows in fact_orders after CDC:\", row_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e9be6-c932-4d15-8f1b-cd4a031a89eb",
   "metadata": {
    "name": "cell20"
   },
   "source": [
    "## 7) Refresh Aggregates After CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503eb50-e9ba-4a94-9167-aa73ab460362",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "\n",
    "fact_orders_post = spark.table(fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"fact_orders\"))\n",
    "\n",
    "agg_revenue_post = (\n",
    "    fact_orders_post.groupBy(\"custkey\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"orderkey\").alias(\"order_cnt\"),\n",
    "        F.sum(\"order_revenue\").alias(\"lifetime_revenue\"),\n",
    "        F.max(\"orderdate\").alias(\"last_order_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "agg_revenue_tbl = fqtn(TARGET_DATABASE, TARGET_SCHEMA, \"agg_customer_revenue\")\n",
    "agg_revenue_post.write.mode(\"overwrite\").format(\"delta\").saveAsTable(agg_revenue_tbl)\n",
    "print(\"Refreshed:\", agg_revenue_tbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d368447-0524-4806-bc36-b46c9c00e4e3",
   "metadata": {
    "name": "cell22"
   },
   "source": [
    "## 8) Runtime (minutes & seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c278076-7567-4fdf-8a95-bc1045566749",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "pipeline_end_ts = datetime.now()\n",
    "elapsed = pipeline_end_ts - pipeline_start_ts\n",
    "\n",
    "mins = int(elapsed.total_seconds() // 60)\n",
    "secs = int(elapsed.total_seconds() % 60)\n",
    "\n",
    "print(f\"Pipeline finished at: {pipeline_end_ts.isoformat(timespec='seconds')}\")\n",
    "print(f\"Total runtime: {mins} min {secs} sec\")\n"
   ]
  }
 ]
}
